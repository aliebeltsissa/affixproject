res.aov <- summary4 %>%
anova_test(scores ~ langexp + condition)
get_anova_table(res.aov)
knitr::opts_chunk$set(echo = TRUE,
eval = TRUE,
warning = FALSE,
fig.path = "graphics/",
fig.width = 8,
fig.height = 4,
fig.retina = 2,
fig.align = "center",
fig.pos = "t",
collapse = TRUE
)
load("data/class5.RData");
library(lme4); #this is necessary for 'lmer'
library(rms); #this is necessary for 'rcs'
package.install(rms)
install.packages("rms")
library(rms); #this is necessary for 'rcs'
library(Hmisc)
install.packages("htmltools")
install.packages("htmltools")
knitr::opts_chunk$set(echo = TRUE,
eval = TRUE,
warning = FALSE,
fig.path = "graphics/",
fig.width = 8,
fig.height = 4,
fig.retina = 2,
fig.align = "center",
fig.pos = "t",
collapse = TRUE
)
load("data/class5.RData");
library(lme4); #this is necessary for 'lmer'
library(rms); #this is necessary for 'rcs'
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
remove.packages("htmltools")
install.packages("htmltools")
install.packages("htmltools")
knitr::opts_chunk$set(echo = TRUE,
eval = TRUE,
warning = FALSE,
fig.path = "graphics/",
fig.width = 8,
fig.height = 4,
fig.retina = 2,
fig.align = "center",
fig.pos = "t",
collapse = TRUE
)
load("data/class5.RData");
library(lme4); #this is necessary for 'lmer'
library(rms); #this is necessary for 'rcs'
m4 <- lmer(rt ~ 1 + rcs(ticks,4)*condition + (1|sbjId) + (1|target), data=data_for_analysis);
summary(m4)[[10]]; #betas are quite crazy (high and low), and quite some variability. Lots of significance, but not easy to interpret. But wait before jumping there, let's check whether the non-linearity was worth it:
anova(m4,m3); #it does provide a nice improvement in goodness of fit, for the extra complexity that it costs (which is a lot, note -- 6 dfs)
#does the model fit the data better?
library(effects);
setwd("C:/Users/annal/OneDrive/Documents/GitHub/affixproject");
participants <- list("5aa787c66219a30001c765f8","5ae5db897edeb000014a85ee","5b212164cfbf200001cfb3ad","5b213220809d160001a2c36d","5bcd11401662020001fe82c1","5c19254b0b9f6b00014769ee","5c1bfabab0fcc900019d9ff4","5caccaed2f93d700157b4782","5cbe02ab39447e0001745a5e","5cefe8856e0eec00015971dd","5d9a125f1714540016a40a27","5dab6e7d80e1780016d5bc9b","5dcb33ab0550ef819f508abf","5e12048c7605fe8839180a14","5e283f8ad48ecf000ddd0724","5e2c6968e97bec29709a53c6","5e3ff59bf2160b23942ada93","5e42b6421f44d3143440b25c","5e42f03607b468000d8eb912","5e433ed3393e971e19f4735d","5e577e79ce30ae13226e61ae","5e6d3324f14d262fced5bbfe","5e87b28549538b0fdf96b318","5e89e89bf025be000c01eead","5e8b66490d48450346bf2755","5e8c57d9b4483e012006e7aa","5e8e55509422bb10abed1f54","5e9bd025eb6b380e1d66d04c","5ea171c1a8782801263e7540","5ea9611edec14d052ada0bae","5ea9cd383b32cf15448a86ac","5eaae42f19a24d05cbd6ccb4","5eaf1c50f3540c614eb973a5","5eb35dff41a381156be161c2","5eb71c59c55b3b0ce571ae7c","5ebc0c153f8b2e04d3833423","5ec12ed7a66dbf01c7d740c1","5ec1c01a8ccaaa5ec0080c91","5ec6d06c67b0da0fb6f85e83","5ecbb4c00bc62403dae2df59","5ecd21dec04cca02c9032485","5ed013f88fce6e0d858b732e","5ee623674afa60375e30eec3","5ee83da8817af0000d47448f","5ee942381a22491bbb7170f8","5eef3de3fb4e031bf546d887","5efd2964d36f63162f263795","5f0471a92ec97b6d1aec7739","5f09d989eff16b266fba44c0","5f0d65ef2ad0c60009209f0f","5f0f93938935de000855898b","5f11c9f84078cd0888177499","5f18a80367ef6b0bbc5f3720","5f19fa0f3e85b20d0336258c","5f1e8dc7e2161c86cdcd220a","5f219602670e5a0af2cf5237","5f33289c1fe9181376ee67d9","5f33c58329beab1f63c0dfaa","5f350f459bf003464a03ea5e","5f47e34858dd331165bf9f00","5f5a713d37f71112463ab4d9","5f5e7de4c81d3672642cd612","5f6cd5ce1758e90f12ae1123","5f708f5f0d32bb66960d4473","5f784f5b98f4963cdf902340","5f7bd801486f5e1ce581980f","5f858743256d25036d9fa693","5f91dc284909fe0b08f9e2e1","5f99cee5a2a7d109a1624e10","5fa4258658811d03fbc6ce8b","5fa4725a186c050edc3bcc20","5fa5793490f10705755818c0","5fa59d4d5a29c90da975efe9","5fc2d2d79da439201ab6addc","5fc44d83862e3e79b02e0438","5fc781689771f106330abf6a","5fd2416e5061f30735e202e9","5fd69a0014c3b25ee539f66b","5fd9c5c67fa7c74ec42bb318","5fdbf74c5474cd11e39eeaf3","5fe2308a91773957e88b89be","600e0dadada7da69afc5b3de","600f0f9c3bfcdc077c924e51","60123adc24c9f126819861b6","601705a0246e51313e8ed38e","601951245f481e000980504f","601b5c79e845ac360826c9df","601ffb441dc6d959b855eea5","602bb123612bfe330818d4ef","6042a56575ab0c1ccffc3152","60491025dd8eb31e48a0ca8d","60536ffae4f98513b671f482","605aa0bb5fb71bbcf0808892","60645fe61129208791535d15","6065d7d47409810344f7e6bb","60664619eac28843177fee96","60670ac0fc7e169231369ab1","606dc21c1ce7dc64af9d81fb","6074768dc48e0753011d3d72","607ea5504e74d104da5a108a","60883a7e91c0be66dbb96fc8","608858869a591bd129fbbc6c","608c2e566d92c2aa3543e2d2","608c64e14968cdd8dcbe1be4","6092e2aac56871319199b6a5","6093cacf9a39751eedd55916","60957b300f08087b0af0031c","60a3fe2d888e7a090d6d8f82","60a68725b2b424dc0d7ce793","60b7b8126c0c4524e913236e","60bbe0da43325dadf2b1b6a9","60c49e68081928c86ba7b98a","60d478e72e8251287b641b2d","60d7605d7af8d66774061717","60db4aed5dd7b87124f51341","60dee3200c7c3355c62326d9","60e12640be1b69b66053d55b","60e3b18d3d4205b4c52ada56","60e9ea97d19e613e60a994da","60eb26fb8db6845a14fe5a91","60ec51c51a3158a50ded8a3e","60edd136d544e7c8f5fac8a7","60f030dca19a06db1b50d16a","60f56f0103876eae7c0d870f","60f579085ebf24f9c80f223e","60f6b8f8e574e14634ead43e","60f88d7fef1959734061e5a7","60fd0f49858465796afd5da9","6100174f92a7a0e5141b195e","61001b6892ebd05a24fe82e1","61055020201a7da5a704f7dd","6107d788163252e3b5c348e3","610834993bac40f050062896","610958b8473c7b96e4bd207d","610a52647a452dee7ca89360","610d2275ba5f1ee0fdcee3e8","610f2eabb45be8717fc155e0","61125e23136464bd2cbbefc9","61143bd22a1649da3b52ae32","611bdb0fed7c9df6dce28c3f","611cebb780fbae98c5bcc84d","611dc53f0098557586e89b17","611e7d1295185bf6f56fd951","611e9c16e90a0d4c6f0d8df5","611eafa6011b0423f7d7230f",
"611eeafa283a2d1f57537fea","6120211d8e1eab16fcb7ad69","6122966a93af160af481836c","6124a4ba3df07e768bd9c90b","61275a3158c67415afb971ff","612a98181f4b6d1cf01f926c","61301fcb49db2f170cb02b93","6131f7010e926c9103558040","613867f34e206e4f573bc6ef","613a73d06cf1fcfef304d3fa","613ab1e64acd61ebd0a7116d","613f57084e80f056c78f9b7b","6140f1c4ead758f1b72bbe2f","6140ff9c5750f0081ca8d71f","6144742e57a61e489cc9e978","6147aed5b2a4f748dc2b6ba4","614b323022fcd8b7408005a6","614c80483d06dcf7ad552679","614c8e5469405530dc27b9d5","614dd6473c74b217ad73cc9c","61520b079436973e05f72d33","61548fa2b637194e04c9bc09","6155e204cc071b306458dfff","6156a440279fdf408ee663ae","6156b68cc77b48d6693b361c","615b042301e3a24311563ee4","615b1ab789b14c9996251fb4","615b41767003d4ece749ed9d","615da9ee365ab7d547b98979","615f371e6688c87e53e7acf5","61616ce2bb94584e11c911af","6163faf9d9ac4586fe776568","616494f1ae4537c01914c758","616631efeeac0979c1ba85a9","616891e25a026e1f7262116c","616eb493bb7e4ab4fa1de8d4","6170821d1e8ffb9c893b28a4","6171349b89a54d4823f9eca1","617142c2a843eef6f8f148b6","6171d922c871ba795b6a4827","6172078b966225960be2a7b5","617679054948369cb824d837","63ee5d8aaee278de46b7d4cc","6455490f7c5f35e4221a810a","64764d56699f097a96ec6e5c","64973d83e4fc64d6ae59c370","64a288758b7c82ff2d18da75","64ef422a4789bd6b6b9042ce","65032a60861e9a17bc1a7444","65089e7ce53888b0b3840c62","650aee87054446f772acdfcc");
library(paletteer);
cols <- paletteer_d("MetBrewer::Degas");
# Testing -----------------------------------------------------------------
# import testing data
data_all_testing <- read.csv("testing_preprocessed_clean.csv",header=T,sep=",");
data_all_testing <- subset(data_all_testing, select = -c(X)) # remove redundant column added by Pavlovia
data_testing <- data_all_testing[data_all_testing$sbj_ID %in% participants,]; # n = 196 participants
data_testing <- data_testing[!data_testing$sbj_ID %in% c('615b41767003d4ece749ed9d', '5e8b66490d48450346bf2755','615b042301e3a24311563ee4'),]
# make some variables factors
data_testing$sbj_ID <- as.factor(data_testing$sbj_ID);
data_testing$task <- as.factor(data_testing$task);
data_testing$item <- as.factor(data_testing$item);
data_testing$testing_condition <- as.factor(data_testing$testing_condition);
data_testing$correct <- as.logical(data_testing$correct);
cols2 <- paletteer_d("palettesForR::Named");
# 0M yes responses boxplot
data_testing_0M_yes <- aggregate(data_testing$observed[data_testing$testing_condition=='0M'], by=list(data_testing$sbj_ID[data_testing$testing_condition=='0M']), FUN = function(x) sum(x == 0));
names(data_testing_0M_yes) <- c("sbj_ID","x_0");
# 0M scores
data_testing_0M_means <- aggregate(data_testing$correct[data_testing$testing_condition=='0M'], list(data_testing$sbj_ID[data_testing$testing_condition=='0M']), FUN=mean, na.rm=TRUE);
names(data_testing_0M_means) <- c("sbj_ID","x_0");
# 1M yes responses boxplot
data_testing_1M_yes <- aggregate(data_testing$observed[data_testing$testing_condition=='1M'], by=list(data_testing$sbj_ID[data_testing$testing_condition=='1M']), FUN = function(x) sum(x == 0));
names(data_testing_1M_yes) <- c("sbj_ID","x_1");
# 1M scores
data_testing_1M_means <- aggregate(data_testing$correct[data_testing$testing_condition=='1M'], list(data_testing$sbj_ID[data_testing$testing_condition=='1M']), FUN=mean, na.rm=TRUE);
names(data_testing_1M_means) <- c("sbj_ID","x_1");
# 2M yes responses boxplot
data_testing_2M_yes <- aggregate(data_testing$observed[data_testing$testing_condition=='2M'], by=list(data_testing$sbj_ID[data_testing$testing_condition=='2M']), FUN = function(x) sum(x == 0));
names(data_testing_2M_yes) <- c("sbj_ID","x_2");
# 2M correct boxplot
data_testing_2M_means <- aggregate(data_testing$correct[data_testing$testing_condition=='2M'], list(data_testing$sbj_ID[data_testing$testing_condition=='2M']), FUN=mean, na.rm=TRUE);
names(data_testing_2M_means) <- c("sbj_ID","x_2");
# 2M - hits only
data_testing_2M_hits_means <- aggregate(data_testing$correct[data_testing$testing_condition=='2M'&data_testing$expected=='0'], list(data_testing$sbj_ID[data_testing$testing_condition=='2M'&data_testing$expected=='0']), FUN=mean, na.rm=TRUE);
names(data_testing_2M_hits_means) <- c("sbj_ID","x_2_hits");
# 2M - correct rejections only
data_testing_2M_rejs_means <- aggregate(data_testing$correct[data_testing$testing_condition=='2M'&data_testing$expected=='1'], list(data_testing$sbj_ID[data_testing$testing_condition=='2M'&data_testing$expected=='1']), FUN=mean, na.rm=TRUE);
names(data_testing_2M_rejs_means) <- c("sbj_ID","x_2_rejs");
# Familiarity -------------------------------------------------------------
data_all_familiarity <- read.csv("familiarity_preprocessed_clean.csv",header=T,sep=",");
data_all_familiarity <- subset(data_all_familiarity, select = -c(X)) # remove redundant column added by Pavlovia
data_familiarity <- data_all_familiarity[data_all_familiarity$sbj_ID %in% participants,]; # n = 196 participants
data_familiarity <- data_familiarity[!data_familiarity$sbj_ID %in% c('615b41767003d4ece749ed9d', '5e8b66490d48450346bf2755','615b042301e3a24311563ee4'),] # same participants excluded as for testing
# make some variables factors
data_familiarity$sbj_ID <- as.factor(data_familiarity$sbj_ID);
data_familiarity$task <- as.factor(data_familiarity$task);
data_familiarity$correct <- as.logical(data_familiarity$correct);
data_familiarity$target <- as.factor(data_familiarity$target);
data_familiarity$confound <- as.factor(data_familiarity$confound);
# familiarity accuracy boxplot
data_familiarity_means <- aggregate(data_familiarity$correct, list(data_familiarity$sbj_ID), FUN=mean);
colnames(data_familiarity_means)[colnames(data_familiarity_means)=="Group.1"]="sbj_ID";
# BLP ---------------------------------------------------------------------
data_all_BLP <- read.csv("BLP_preprocessed.csv",header=T,sep=",");
data_all_BLP <- subset(data_all_BLP, select = -c(X)) # remove redundant column added by Pavlovia
data_BLP <- data_all_BLP[data_all_BLP$sbj_ID %in% participants,]; # n = 196 participants
data_BLP <- data_BLP[!data_BLP$sbj_ID %in% c('615b41767003d4ece749ed9d','5e8b66490d48450346bf2755','615b042301e3a24311563ee4'),] # same participants excluded as for testing
data_BLP <- subset(data_BLP, select = -c(AoAgioL1, AoAgioL2, AoAgioL3, AoAgioL4, anniInstrL1, anniInstrL2, anniInstrL3, anniInstrL4, anniPaeseL1, anniPaeseL2, anniPaeseL3, anniPaeseL4, anniFamigliaL1, anniFamigliaL2, anniFamigliaL3, anniFamigliaL4, anniLavoroL1, anniLavoroL2, anniLavoroL3, anniLavoroL4, PercAmiciL1, PercAmiciL2, PercAmiciL3, PercAmiciL4, PercFamigliaL1, PercFamigliaL2, PercFamigliaL3, PercFamigliaL4, PercLavoroL1, PercLavoroL2, PercLavoroL3, PercLavoroL4, PercStessoL1, PercStessoL2, PercStessoL3, PercStessoL4, PercCalcoliL1, PercCalcoliL2, PercCalcoliL3, PercCalcoliL4, ProfParlaL1, ProfParlaL2, ProfParlaL3, ProfParlaL4, ProfCapisceL1, ProfCapisceL2, ProfCapisceL3, ProfCapisceL4, ProfLeggeL1, ProfLeggeL2, ProfLeggeL3, ProfLeggeL4, ProfScriveL1, ProfScriveL2, ProfScriveL3, ProfScriveL4, AttMiStessoL1, AttMiStessoL2, AttMiStessoL3, AttMiStessoL4, AttCulturaL1, AttCulturaL2, AttCulturaL3, AttCulturaL4, AttLivNativoL1, AttLivNativoL2, AttLivNativoL3, AttLivNativoL4, AttMadrelinguaL1, AttMadrelinguaL2, AttMadrelinguaL3, AttMadrelinguaL4));
# standardise language responses
data_BLP[data_BLP == "polish"|data_BLP == "POLISH"] <- "Polish";
data_BLP[data_BLP == "portuguese"|data_BLP == "Portugal"] <- "Portuguese";
data_BLP[data_BLP == "italian"] <- "Italian";
data_BLP[data_BLP == "spanish"] <- "Spanish";
data_BLP[data_BLP == "greek"|data_BLP=="Greece"] <- "Greek";
data_BLP[data_BLP == "french"|data_BLP=="fRANCH"] <- "French";
data_BLP[data_BLP == "arabic"] <- "Arabic";
data_BLP[data_BLP == "ENGLISH"|data_BLP == "english"|data_BLP=="english "|data_BLP == "englis"|data_BLP == "eanglish"|data_BLP == "Enlish"] <- "English";
data_BLP[data_BLP == "xhosa"|data_BLP=="XHOSA"] <- "Xhosa";
data_BLP[data_BLP == "tshivenda"] <- "Tshivenda";
data_BLP[data_BLP == "SETSWANA"] <- "Setswana";
data_BLP[data_BLP == "zulu"] <- "Zulu";
data_BLP[data_BLP == "TSWANA"] <- "Tswana";
data_BLP[data_BLP == "sotho"] <- "Sotho";
data_BLP[data_BLP == "SHONA"] <- "Shona";
data_BLP[data_BLP == "hungarian"] <- "Hungarian";
data_BLP[data_BLP == "afrikaans"|data_BLP=="AFRIKAANS"] <- "Afrikaans";
data_BLP[data_BLP == "german"|data_BLP=="GERMAN"|data_BLP=="germany"|data_BLP=="Deustch"] <- "German";
data_BLP[data_BLP == "sweedish"] <- "Swedish";
data_BLP[data_BLP == "Deutch"] <- "Dutch"; # probably - maybe Deutsch?
data_BLP[data_BLP == "SESOTHO"] <- "Sesotho";
data_BLP[data_BLP == "RUSSIAN"|data_BLP=="russian"] <- "Russian";
data_BLP[data_BLP == "tswana"] <- "Tswana";
data_BLP[data_BLP == "SEPEDI"|data_BLP=="sepedi"] <- "Sepedi";
data_BLP[data_BLP == "XItsonga"] <- "Xitsonga";
data_BLP[data_BLP == "N/A"] <- "n/a";
# correcting some participants' demographic information - correction based off of Prolific's information
data_BLP["Age"][data_BLP["sbj_ID"] == "5aa787c66219a30001c765f8"] <- "24";
data_BLP["Gender"][data_BLP["sbj_ID"] == "5aa787c66219a30001c765f8"] <- "Man";
data_BLP["Gender"][data_BLP["sbj_ID"] == "5bcd11401662020001fe82c1"] <- "Man";
data_BLP["Gender"][data_BLP["sbj_ID"] == "5e3ff59bf2160b23942ada93"] <- "Man";
data_BLP["Age"][data_BLP["sbj_ID"] == "5e577e79ce30ae13226e61ae"] <- "24";
data_BLP["Age"][data_BLP["sbj_ID"] == "5e8c57d9b4483e012006e7aa"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "5ecbb4c00bc62403dae2df59"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "5ee942381a22491bbb7170f8"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "5f219602670e5a0af2cf5237"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "5f47e34858dd331165bf9f00"] <- "21";
data_BLP["Gender"][data_BLP["sbj_ID"] == "5f5a713d37f71112463ab4d9"] <- "Woman";
data_BLP["Age"][data_BLP["sbj_ID"] == "5f99cee5a2a7d109a1624e10"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "5fa4725a186c050edc3bcc20"] <- "23";
data_BLP["Age"][data_BLP["sbj_ID"] == "5fc2d2d79da439201ab6addc"] <- "24";
data_BLP["Age"][data_BLP["sbj_ID"] == "5fc44d83862e3e79b02e0438"] <- "24";
data_BLP["Age"][data_BLP["sbj_ID"] == "5fd9c5c67fa7c74ec42bb318"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "6065d7d47409810344f7e6bb"] <- "24";
data_BLP["Age"][data_BLP["sbj_ID"] == "60883a7e91c0be66dbb96fc8"] <- "23";
data_BLP["Age"][data_BLP["sbj_ID"] == "608c2e566d92c2aa3543e2d2"] <- "22";
data_BLP["Age"][data_BLP["sbj_ID"] == "60e3b18d3d4205b4c52ada56"] <- "24";
data_BLP["Gender"][data_BLP["sbj_ID"] == "60e9ea97d19e613e60a994da"] <- "Man";
data_BLP["Age"][data_BLP["sbj_ID"] == "611eafa6011b0423f7d7230f"] <- "20";
data_BLP["Age"][data_BLP["sbj_ID"] == "612a98181f4b6d1cf01f926c"] <- "21";
data_BLP["Age"][data_BLP["sbj_ID"] == "61301fcb49db2f170cb02b93"] <- "25";
data_BLP["Age"][data_BLP["sbj_ID"] == "6156b68cc77b48d6693b361c"] <- "21";
data_BLP["Age"][data_BLP["sbj_ID"] == "616891e25a026e1f7262116c"] <- "22";
data_BLP["Gender"][data_BLP["sbj_ID"] == "616eb493bb7e4ab4fa1de8d4"] <- "Woman";
data_BLP["Age"][data_BLP["sbj_ID"] == "6171d922c871ba795b6a4827"] <- "23";
# make some variables factors
data_BLP$task <- as.factor(data_BLP$task)
data_BLP$sbj_ID <- as.factor(data_BLP$sbj_ID);
data_BLP$Age <- as.numeric(data_BLP$Age);
data_BLP$Gender <- as.factor(data_BLP$Gender);
data_BLP$Education <- as.factor(data_BLP$Education);
data_BLP$L1 <- as.factor(data_BLP$L1);
data_BLP$L2 <- as.factor(data_BLP$L2);
data_BLP$L3 <- as.factor(data_BLP$L3);
data_BLP$L4 <- as.factor(data_BLP$L4);
data_BLP$otherLs <- as.factor(data_BLP$otherLs);
data_BLP$AttentionL1 <- as.factor(data_BLP$AttentionL1);
data_BLP$AttentionL2 <- as.factor(data_BLP$AttentionL2);
data_BLP$AttentionL3 <- as.factor(data_BLP$AttentionL3);
data_BLP$AttentionL4 <- as.factor(data_BLP$AttentionL4);
summary(data_BLP);
library(toolbox);
scores_list <- combineCols(data_BLP, cols=c('L1Score','L2Score','L3Score','L4Score'),by_name=TRUE); # combine scores into 1 list
data_BLP$temp_sbjID <- c(1:193); # necessary: R doesn't like format of Prolific IDs
# multilingual balance: variance
vars <- list();
for (i in 1:193) { # calculate variance for each participant
temp <- unlist(scores_list[i]);
var <- var(temp,na.rm=TRUE);
vars <- append(vars, var)
};
data_BLP$lang_var <- vars;
data_BLP$lang_var <- as.numeric(data_BLP$lang_var);
# multilingual balance: entropy
entropies <- list();
library(DescTools);
for (i in 1:193) { # calculate entropy for each participant
temp <- unlist(scores_list[i]);
entropy <- Entropy(temp,na.rm=TRUE);
entropies <- append(entropies, entropy)
};
data_BLP$lang_ent <- entropies;
data_BLP$lang_ent <- as.numeric(data_BLP$lang_ent);
# multilingual experience: summing all language scores
data_BLP["L2Score"][is.na(data_BLP["L2Score"])] <- 0;
data_BLP["L3Score"][is.na(data_BLP["L3Score"])] <- 0;
data_BLP["L4Score"][is.na(data_BLP["L4Score"])] <- 0;
data_BLP$multi_exp <- data_BLP$L1Score + data_BLP$L2Score + data_BLP$L3Score + data_BLP$L4Score;
# L1 - L2 score
data_BLP$L1_L2_diff <- data_BLP$L1Score - data_BLP$L2Score;
# CLUSTERING #
complete_cases <- complete.cases(data_BLP)
data_filtered <- data_BLP[complete_cases, ]
#without language dominance scores
pca_varimax2 <- psych::principal(data_BLP[,19:34], nfactors=16, rotate='varimax');
data_BLP <- cbind(data_BLP, pca_varimax2$scores[,c('RC1','RC9','RC2','RC6')]);
names(data_BLP)[116:119] <- c('RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4');
# adding testing scores and BLP metrics together
library(tidyverse);
data_BLP_extracted_all <- subset(data_BLP, select=c(sbj_ID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score,lang_var,lang_ent,multi_exp,L1_L2_diff,RC1_L3,RC9_L4,RC2_use_L1vsL2,RC6_use_L4));
data_BLP_testing_all <- list(data_testing_2M_means,data_testing_2M_hits_means,data_testing_2M_rejs_means,data_BLP_extracted_all) %>% reduce(inner_join, by='sbj_ID');
# 1M & 2M clustering
data_testing_1M2M_means <- merge(data_testing_1M_means,data_testing_2M_means,by.x='sbj_ID',by.y='sbj_ID');
data_BLP_testing_1M2M_means <- merge(data_testing_1M2M_means, data_BLP_extracted_all[,c('sbj_ID','lang_ent','multi_exp','L1_L2_diff','RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4')], by.x='sbj_ID',by.y='sbj_ID', all.x=T);
data_testing_1M2M_yes <- merge(data_testing_1M_yes,data_testing_2M_yes,by.x='sbj_ID',by.y='sbj_ID');
data_BLP_testing_1M2M_yes <- merge(data_testing_1M2M_yes, data_BLP_extracted_all[,c('sbj_ID','lang_ent','multi_exp','L1_L2_diff','RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4')], by.x='sbj_ID',by.y='sbj_ID', all.x=T);
################
# LINEAR MODEL #
################
library(lme4);
# TESTING #
data_testing_lm <- merge(data_testing, data_BLP[,c('temp_sbjID','sbj_ID','Gender','Age','lang_ent','multi_exp','L1_L2_diff','RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4')], by.x='sbj_ID',by.y='sbj_ID', all.x=T);
#2M - accuracy
data_testing_lm_2M <- subset(data_testing_lm[data_testing$testing_condition=='2M',]);
# FAMILIARITY #
data_BLP_familiarity <- merge(data_familiarity, data_BLP_extracted_all[,c('sbj_ID','lang_ent','multi_exp','L1_L2_diff','RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4')], by.x='sbj_ID',by.y='sbj_ID', all.x=T);
# check - if entropy includes just 3 languages, do we see the same effect as
#with L1_L2_diff above?
scores_list_just3 <- combineCols(data_BLP, cols=c('L1Score','L2Score','L3Score'),by_name=TRUE); # combine scores into 1 list
entropies_just3 <- list();
for (i in 1:193) { # calculate entropy for each participant
temp <- unlist(scores_list_just3[i]);
entropy <- Entropy(temp,na.rm=TRUE);
entropies_just3 <- append(entropies_just3, entropy)
};
data_BLP$lang_ent_just3 <- entropies_just3;
data_BLP$lang_ent_just3 <- as.numeric(data_BLP$lang_ent_just3);
data_ent_lm <- merge(data_testing, data_BLP[,c('sbj_ID','lang_ent_just3')], by.x='sbj_ID',by.y='sbj_ID', all.x=T);
m23 <- glmer(observed ~ scale(trialn) + testing_condition*lang_ent_just3 + (1+testing_condition|sbj_ID), data=subset(data_ent_lm, rt>300 & rt<3000), family='binomial');
######################
# DENSITY CLUSTERING #
######################
library(viridis);
density_peak_clustering <- function(scores,
sbjId, #these are the "objects"
dimensions, #this would be the item in a psychological experiment
threshold=.2
)
{
#this just checks that the scores are numbers
if (!is.numeric(scores)) stop('The score vector should be numeric');
#this arranges the input data into a dataframe, in the wide format cause that's what the function 'dist', which we'll use below, needs
temp <- data.frame(dimension=dimensions, sbjId=sbjId, score=scores);
tempWide <- reshape(temp, timevar="dimension", idvar="sbjId", direction="wide");
#this method doesn't handle well missing data, so here I substitute them with the mean for that dimension
for (j in 2:ncol(tempWide))
{
tempWide[,j][is.na(tempWide[,j])] <- rep(mean(tempWide[,j], na.rm=T), length(tempWide[,j][is.na(tempWide[,j])]));
};
#this prepares the core data frame:
density_peaks <- data.frame(subject=tempWide$sbjId, ro=rep(0,length(tempWide$sbjId)), delta=rep(0,length(tempWide$sbjId)), cluster=rep(0,length(tempWide$sbjId)));
#this computes the distance between points
distances <- as.matrix(dist(tempWide[,2:ncol(tempWide)]));
rownames(distances) <- tempWide$sbjId;
colnames(distances) <- tempWide$sbjId;
#this computes the density for each point, ro in R&L2014 terminology
temp <- ifelse(distances > threshold*median(distances, na.rm=T), 0, 1); #here we take the median of the distance distribution as a reference point, but the method should be robust with respect to this arbitrary choice
density_peaks$ro <- rowSums(temp, na.rm=T);
#this computes the distance to the closest, higher-density point
for(j in 1:nrow(density_peaks)) density_peaks$delta[j] <- min(distances[j,which(density_peaks$ro>density_peaks$ro[j])]);
#delta is 'Inf' by definition for the highest-density point, so we change it with max(delta)
density_peaks$delta[density_peaks$ro==max(density_peaks$ro)] <- max(density_peaks$delta[density_peaks$delta!=Inf], na.rm=T);
#plot the decision plot
with(density_peaks, plot(jitter(ro,2), jitter(delta,2), type="n", axes=F, xlab='Number of relative neighbours (ro)', ylab='Minimal distance to higher density point (delta)'));
axis(1);
axis(2);
with(density_peaks, text(jitter(ro,2), jitter(delta,2), as.character(subject)));
#ask the user to identify the cluster centres
print("Pick up your cluster centres. Press ENTER when done.");
scan(what="character") ->> centres;
#assign the cluster centres their cluster ID
clusterCounter <- 1;
for (j in 1:length(centres))
{
density_peaks$cluster[density_peaks$subject==centres[j]] <- clusterCounter;
clusterCounter <- clusterCounter+1;
};
#assign all the other points their cluster ID
unassignedPoints <- which(density_peaks$cluster==0);
unassignedPoints <- unassignedPoints[order(density_peaks$ro[unassignedPoints], decreasing=T)]; #here I order the unassigned points based on their density. This eliminates the risk that points are passed through the cluster assignment algorithm when its neighbours are still all unassigned themselves.
for (j in unassignedPoints)
{
nearestHigherDensityNeighbour <- which(distances[j,] == min(distances[j,which(density_peaks$ro>density_peaks$ro[j])]));
density_peaks[j,'cluster'] <- density_peaks[nearestHigherDensityNeighbour,'cluster'];
}
#this plots the points, color-coded by cluster, in a compressed, 2D space (via Multidimensional scaling)
temp <- cmdscale(distances, k=2);
plot(temp[,1], temp[,2], bty='n', xlab='(Multidimensional scaling)', ylab='', main='', type='n');
cluster_colors <- viridis(length(centres));
for (j in 1:length(centres)) text(temp[density_peaks$cluster==j,1], temp[density_peaks$cluster==j,2], density_peaks$subject[density_peaks$cluster==j], col=cluster_colors[j]);
# create the dataframe with delta, ro, and sbjId
result <- data.frame(cluster = density_peaks$cluster, delta = density_peaks$delta, ro = density_peaks$ro, sbjId = density_peaks$subject)
# return the result
return(result)
}
data_BLP_clustering <- subset(data_BLP, select=c(temp_sbjID,RC1_L3,RC9_L4,RC2_use_L1vsL2,RC6_use_L4));
data_clustering <- data.frame();
sbj_ID <- as.character(data_BLP_clustering$temp_sbjID);
for (x in 1:193) {
temp_sbj_ID = sbj_ID[x]
RC1_L3 = data_BLP_clustering[x,2]
RC9_L4 = data_BLP_clustering[x,3]
RC2_use_L1vsL2 = data_BLP_clustering[x,4]
RC6_use_L4 = data_BLP_clustering[x,5]
temp <- data.frame('sbj_ID'=rep(temp_sbj_ID,4),
'scores'=c(RC1_L3,RC9_L4,RC2_use_L1vsL2,RC6_use_L4),
'dimensions'=c('RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4'))
data_clustering <- rbind(data_clustering,temp)
};
result <- density_peak_clustering(data_clustering$scores,data_clustering$sbj_ID,data_clustering$dimensions);
# new clustering analysis based on new version of Laio clustering technique
#getting all the participant IDs together
new_cluster1 <- list('2','3','4','5','6','7','8','10','12','13','14','15','16','19','20','21','22','23','24','25','27','28','29','31','32','33','35','36','38','39','41','43','44','45','46','47','48','49','50',
'51','52','53','55','56','57','58','59','60','61','63','64','66','67','68','69','72','73','74','75','76','77','78','79','81','82','83','84','87','88','89','94','95','100',
'101','102','106','107','108','110','113','114','115','117','118','120','124','125','127','128','131','132','134','135','138','141','142','143','144','145','146','148','149','150',
'153','155','156','157','158','159','160','161','162','163','165','166','167','170','171','174','176','177','179','182','183','185','187','188','189','191');
new_cluster2 <- list('11','26','34','37','42','54','62','65','92','98','105','116','123','130','136','137','139','147','169','173','175','178','190');
new_cluster3 <- list('1','9','17','18','30','40','70','71','80','86','90','96','97','99','103','111','112','126','133','152','154','186','192','193');
new_cluster4 <- list('85','93','104','109','121','122','129','140','151','172','180','181');
new_cluster5 <- list('91','119','164','168','184');
#creating separate dataframes
cluster1 <- subset(data_BLP[data_BLP$temp_sbjID %in% new_cluster1,],select=c(temp_sbjID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score));
# n = 129
cluster2 <- subset(data_BLP[data_BLP$temp_sbjID %in% new_cluster2,],select=c(temp_sbjID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score));
# n = 23
cluster3 <- subset(data_BLP[data_BLP$temp_sbjID %in% new_cluster3,],select=c(temp_sbjID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score));
# n = 24
cluster4 <- subset(data_BLP[data_BLP$temp_sbjID %in% new_cluster4,],select=c(temp_sbjID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score));
# n = 12
cluster5 <- subset(data_BLP[data_BLP$temp_sbjID %in% new_cluster5,],select=c(temp_sbjID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score));
#getting average of scores to find dimension pulling clusters apart
cluster1_means <- data.frame('cluster_id'='1');
for (x in 2:21) {
name <- colnames(cluster1)[x];
mean <- mean(cluster1[,x]);
cluster1_means[name] <- mean;
};
cluster2_means <- data.frame('cluster_id'='2');
for (x in 2:21) {
name <- colnames(cluster2)[x];
mean <- mean(cluster2[,x]);
cluster2_means[name] <- mean;
};
cluster3_means <- data.frame('cluster_id'='3');
for (x in 2:21) {
name <- colnames(cluster3)[x];
mean <- mean(cluster3[,x]);
cluster3_means[name] <- mean;
};
cluster4_means <- data.frame('cluster_id'='4');
for (x in 2:21) {
name <- colnames(cluster4)[x];
mean <- mean(cluster4[,x]);
cluster4_means[name] <- mean;
};
cluster5_means <- data.frame('cluster_id'='5');
for (x in 2:21) {
name <- colnames(cluster5)[x];
mean <- mean(cluster5[,x]);
cluster5_means[name] <- mean;
};
cluster_means <- rbind(cluster1_means, cluster2_means, cluster3_means, cluster4_means, cluster5_means);
# testing scores per cluster
data_BLP_extracted_all <- subset(data_BLP, select=c(temp_sbjID,sbj_ID,HistoryL1Score,HistoryL2Score,HistoryL3Score,HistoryL4Score,UseL1Score,UseL2Score,UseL3Score,UseL4Score,ProficiencyL1Score,ProficiencyL2Score,ProficiencyL3Score,ProficiencyL4Score,AttitudeL1Score,AttitudeL2Score,AttitudeL3Score,AttitudeL4Score,L1Score,L2Score,L3Score,L4Score,lang_var,lang_ent,multi_exp,L1_L2_diff,RC1_L3,RC9_L4,RC2_use_L1vsL2,RC6_use_L4));
data_BLP_extracted_all$cluster <- "1";
data_BLP_extracted_all$cluster[data_BLP_extracted_all$temp_sbjID %in% new_cluster2] <- "2";
data_BLP_extracted_all$cluster[data_BLP_extracted_all$temp_sbjID %in% new_cluster3] <- "3";
data_BLP_extracted_all$cluster[data_BLP_extracted_all$temp_sbjID %in% new_cluster4] <- "4";
data_BLP_extracted_all$cluster[data_BLP_extracted_all$temp_sbjID %in% new_cluster5] <- "5";
data_BLP_extracted_all$cluster <- as.factor(data_BLP_extracted_all$cluster);
data_BLP_testing_all <- list(data_testing_0M_means,data_testing_1M_means,data_testing_2M_means,data_BLP_extracted_all) %>% reduce(inner_join, by='sbj_ID');
# group boxplot - yes responses
data_BLP_testing_all_yes <- list(data_testing_2M_yes,data_BLP_extracted_all) %>% reduce(inner_join, by='sbj_ID');
data_BLP_testing_all_yes$x_2 <- data_BLP_testing_all_yes$x_2 * 2.5
# familiarity responses in each cluster
data_BLP_familiarity_all <- list(data_familiarity_means,data_BLP_extracted_all) %>% reduce(inner_join, by='sbj_ID');
collapsed_familiarity_all <- subset(data_BLP_familiarity_all, select=c(sbj_ID,x,cluster));
# cluster lmer
data_testing_lm$cluster <- "1";
data_testing_lm$cluster[data_testing_lm$temp_sbjID %in% new_cluster2] <- "2";
data_testing_lm$cluster[data_testing_lm$temp_sbjID %in% new_cluster3] <- "3";
data_testing_lm$cluster[data_testing_lm$temp_sbjID %in% new_cluster4] <- "4";
data_testing_lm$cluster[data_testing_lm$temp_sbjID %in% new_cluster5] <- "5";
data_testing_lm$cluster <- as.factor(data_testing_lm$cluster);
#all testing conditions - 'yes' responses
m26 <- glmer(observed ~ scale(trialn) + testing_condition*cluster + (1+testing_condition|sbj_ID), data=subset(data_testing_lm, rt>300 & rt<3000), family='binomial');
#all testing conditions - 'yes' responses
m27 <- glmer(observed ~ scale(trialn) + testing_condition*cluster + (1+testing_condition|sbj_ID), data=subset(data_testing_lm, rt>300 & rt<3000), family='binomial');
#2M accuracy
data_testing_lm_2M <- subset(data_testing_lm[data_testing$testing_condition=='2M',]);
m28 <- glmer(observed ~ scale(trialn) + expected*cluster + (1+expected|sbj_ID), data=data_testing_lm_2M, family='binomial');
#familiarity
data_BLP_familiarity <- merge(data_familiarity, data_BLP_extracted_all[,c('sbj_ID','cluster','lang_ent','multi_exp','L1_L2_diff','RC1_L3','RC9_L4','RC2_use_L1vsL2','RC6_use_L4')], by.x='sbj_ID',by.y='sbj_ID', all.x=T);
m29 <- glmer(correct ~ scale(trialn) + cluster + (1|sbj_ID), data=data_BLP_familiarity, family='binomial');
# examination of significant cluster5*1M interaction
data_BLP_testing_0M_cluster_yes <- merge(data_testing_0M_yes, subset(data_BLP_extracted_all,select=c('sbj_ID','cluster')), by.x='sbj_ID',by.y='sbj_ID', all.x=T);
data_BLP_testing_1M_cluster_yes <- merge(data_testing_1M_yes, subset(data_BLP_extracted_all,select=c('sbj_ID','cluster')), by.x='sbj_ID',by.y='sbj_ID', all.x=T);
data_BLP_testing_2M_cluster_yes <- merge(data_testing_2M_yes, subset(data_BLP_extracted_all,select=c('sbj_ID','cluster')), by.x='sbj_ID',by.y='sbj_ID', all.x=T);
df_list <- list(data_BLP_testing_0M_cluster_yes[data_BLP_testing_0M_cluster_yes$cluster==5,],data_BLP_testing_1M_cluster_yes[data_BLP_testing_1M_cluster_yes$cluster==5,],data_BLP_testing_2M_cluster_yes[data_BLP_testing_2M_cluster_yes$cluster==5,]);
data_BLP_testing_cluster5_yes <- df_list %>% reduce(full_join, by='sbj_ID');
summary(m26); # lang_ent_just3 non sig, interaction non sig
summary(m23); # lang_ent_just3 non sig, interaction non sig
summary(m23); # lang_ent_just3 non sig, interaction non sig
plot(data_BLP_testing_0M_L1L2_yes$L1_L2_diff,data_BLP_testing_0M_L1L2_yes$x_0,xlab="L1-L2 difference",ylab="0M 'yes' responses",ylim=c(0,40),pch=19);
abline(lm(data_BLP_testing_0M_L1L2_yes$x_0~data_BLP_testing_0M_L1L2_yes$L1_L2_diff), col = "red",lwd=2);
#L1_L2_diff
data_BLP_testing_0M_L1L2_yes <- merge(data_testing_0M_yes, subset(data_BLP,select=c('sbj_ID','L1_L2_diff')), by.x='sbj_ID',by.y='sbj_ID', all.x=T);
data_BLP_testing_1M_L1L2_yes <- merge(data_testing_1M_yes, subset(data_BLP,select=c('sbj_ID','L1_L2_diff')), by.x='sbj_ID',by.y='sbj_ID', all.x=T);
data_BLP_testing_2M_L1L2_yes <- merge(data_testing_2M_yes, subset(data_BLP,select=c('sbj_ID','L1_L2_diff')), by.x='sbj_ID',by.y='sbj_ID', all.x=T);
plot(data_BLP_testing_0M_L1L2_yes$L1_L2_diff,data_BLP_testing_0M_L1L2_yes$x_0,xlab="L1-L2 difference",ylab="0M 'yes' responses",ylim=c(0,40),pch=19);
abline(lm(data_BLP_testing_0M_L1L2_yes$x_0~data_BLP_testing_0M_L1L2_yes$L1_L2_diff), col = "red",lwd=2);
plot(data_BLP_testing_1M_L1L2_yes$L1_L2_diff,data_BLP_testing_1M_L1L2_yes$x_1,xlab="L1-L2 difference",ylab="1M 'yes' responses",ylim=c(0,40),pch=19);
abline(lm(data_BLP_testing_1M_L1L2_yes$x_1~data_BLP_testing_1M_L1L2_yes$L1_L2_diff), col = "red",lwd=2);
plot(data_BLP_testing_2M_L1L2_yes$L1_L2_diff,data_BLP_testing_2M_L1L2_yes$x_2,xlab="L1-L2 difference",ylab="2M 'yes' responses",ylim=c(0,40),pch=19);
abline(lm(data_BLP_testing_2M_L1L2_yes$x_2~data_BLP_testing_2M_L1L2_yes$L1_L2_diff), col = "red",lwd=2);
library(effects);
L1_L2_diff_values <- c(0,quantile(data_testing_lm$L1_L2_diff, seq(.25,.75,.25)));
L1_L2_diff_predictions <- data.frame(Effect(mod=m10, focal.predictors=c('testing_condition','L1_L2_diff'), xlevels=list(L1_L2_diff=L1_L2_diff_values)));
with(subset(L1_L2_diff_predictions, testing_condition=='0M'),
plot(L1_L2_diff, fit, type='b', ylim=c(.2,.6),col='black'));
m10 <- glmer(observed ~ scale(trialn) + expected*Gender + (1+expected|sbj_ID), data=data_testing_lm_2M, family='binomial');
m9 <- glmer(observed ~ scale(trialn) + testing_condition*scale(L1_L2_diff) + (1+testing_condition|sbj_ID), data=subset(data_testing_lm, rt>300 & rt<3000), family='binomial');
L1_L2_diff_values <- c(0,quantile(data_testing_lm$L1_L2_diff, seq(.25,.75,.25)));
L1_L2_diff_predictions <- data.frame(Effect(mod=m9, focal.predictors=c('testing_condition','L1_L2_diff'), xlevels=list(L1_L2_diff=L1_L2_diff_values)));
with(subset(L1_L2_diff_predictions, testing_condition=='0M'),
plot(L1_L2_diff, fit, type='b', ylim=c(.2,.6),col='black'));
with(subset(L1_L2_diff_predictions, testing_condition=='0M'),
polygon(c(L1_L2_diff, L1_L2_diff[4:1]), c(upper,lower[4:1]), col=rgb(t(col2rgb(cols[2])/255),alpha=0.5)));
with(subset(L1_L2_diff_predictions, testing_condition=='1M'),
lines(L1_L2_diff, fit, type='b', lty=2,col='black'));
with(subset(L1_L2_diff_predictions, testing_condition=='1M'),
polygon(c(L1_L2_diff, L1_L2_diff[4:1]), c(upper,lower[4:1]), col=rgb(t(col2rgb(cols[3])/255),alpha=0.5)));
with(subset(L1_L2_diff_predictions, testing_condition=='2M'),
lines(L1_L2_diff, fit, type='b', lty=3,col='black'));
with(subset(L1_L2_diff_predictions, testing_condition=='2M'),
polygon(c(L1_L2_diff, L1_L2_diff[4:1]), c(upper,lower[4:1]), col=rgb(t(col2rgb(cols[4])/255),alpha=0.5)));
with(subset(L1_L2_diff_predictions, testing_condition=='0M'),
plot(L1_L2_diff, fit, type='b', ylim=c(.2,.6),col='black'));
with(subset(L1_L2_diff_predictions, testing_condition=='0M'),
polygon(c(L1_L2_diff, L1_L2_diff[4:1]), c(upper,lower[4:1]), col=rgb(t(col2rgb(cols[2])/255),alpha=0.5)));
with(subset(L1_L2_diff_predictions, testing_condition=='1M'),
lines(L1_L2_diff, fit, type='b', lty=2,col='black'));
with(subset(L1_L2_diff_predictions, testing_condition=='1M'),
polygon(c(L1_L2_diff, L1_L2_diff[4:1]), c(upper,lower[4:1]), col=rgb(t(col2rgb(cols[3])/255),alpha=0.5)));
with(subset(L1_L2_diff_predictions, testing_condition=='2M'),
lines(L1_L2_diff, fit, type='b', lty=3,col='black'));
with(subset(L1_L2_diff_predictions, testing_condition=='2M'),
polygon(c(L1_L2_diff, L1_L2_diff[4:1]), c(upper,lower[4:1]), col=rgb(t(col2rgb(cols[4])/255),alpha=0.5)));
summary(data_BLP$L1_L2_diff)
boxplot(data_BLP_testing_all$x_0 ~ data_BLP_testing_all$cluster,xlab='Cluster',ylab='2M Accuracy',cex.lab=1.5,ylim=c(0,1),yaxs="i");
abline(h=0.5, lty=5); # all at 50% mean, some fluctuation across groups
boxplot(data_BLP_testing_all$x_1 ~ data_BLP_testing_all$cluster,xlab='Cluster',ylab='1M Accuracy',cex.lab=1.5,ylim=c(0,1),yaxs="i");
abline(h=0.5, lty=5); # most above 50%, cluster 5 below (but cluster 5 n = 5)
boxplot(data_BLP_testing_all$x_2 ~ data_BLP_testing_all$cluster,xlab='Cluster',ylab='2M Accuracy',cex.lab=1.5,ylim=c(0,1),yaxs="i");
abline(h=0.5, lty=5); # all at 50% mean, quite even
boxplot(data_BLP_testing_all_yes$x_2 ~ data_BLP_testing_all_yes$cluster,xlab='Cluster',ylab='Percent of yes responses - 2M',cex.lab=1.5,ylim=c(0,100),yaxs="i");
abline(h=50, lty=5); # all at 65%, except cluster 5 (but cluster 5 n = 5)
boxplot(collapsed_familiarity_all$x ~ collapsed_familiarity_all$cluster,xlab='Cluster',ylab='Familiarity accuracy',cex.lab=1.5,ylim=c(0,1),yaxs="i");
abline(h=0.5, lty=5); # all at 57% roughly
summary(m27); # cluster non significant as main effect, 1M*cluster5 sig (p < 0.01), 2M*cluster5 sig (p < 0.01)
summary(m27); # cluster non significant as main effect, 1M*cluster5 sig (p < 0.01), 2M*cluster5 sig (p < 0.01)
summary(m28); # cluster non sig
summary(m29); # cluster non sig
plot(data_BLP_testing_0M_cluster_yes$cluster,data_BLP_testing_0M_cluster_yes$x_0,xlab="Cluster",ylab="0M 'yes' responses",ylim=c(0,40),pch=19);
abline(h=20,lty=5);
plot(data_BLP_testing_1M_cluster_yes$cluster,data_BLP_testing_1M_cluster_yes$x_1,xlab="Cluster",ylab="1M 'yes' responses",ylim=c(0,40),pch=19);
abline(h=20,lty=5);
plot(data_BLP_testing_2M_cluster_yes$cluster,data_BLP_testing_2M_cluster_yes$x_2,xlab="Cluster",ylab="2M 'yes' responses",ylim=c(0,40),pch=19);
abline(h=20,lty=5);
plot(data_BLP_testing_0M_cluster_yes$cluster,data_BLP_testing_0M_cluster_yes$x_0,xlab="Cluster",ylab="0M 'yes' responses",ylim=c(0,40),pch=19);
abline(h=20,lty=5);
plot(data_BLP_testing_1M_cluster_yes$cluster,data_BLP_testing_1M_cluster_yes$x_1,xlab="Cluster",ylab="1M 'yes' responses",ylim=c(0,40),pch=19);
abline(h=20,lty=5);
plot(data_BLP_testing_2M_cluster_yes$cluster,data_BLP_testing_2M_cluster_yes$x_2,xlab="Cluster",ylab="2M 'yes' responses",ylim=c(0,40),pch=19);
abline(h=20,lty=5);

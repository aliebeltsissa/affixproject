#stats on rejected DP at this stage
length(data3fam$familiarityStd[!is.na(data3fam$familiarityStd)]); #total number of valid datapoints before clustering (23501)
length(data4fam$familiarityStd[!is.na(data4fam$familiarityStd)]); #total number of valid datapoints now (22994)
round((length(data3fam$familiarityStd[!is.na(data3fam$familiarityStd)]) - length(data4fam$familiarityStd[!is.na(data4fam$familiarityStd)])) / length(data3fam$familiarityStd[!is.na(data3fam$familiarityStd)])*100, digits=2); #2.16% fam
round((length(data3aoa$ageOfAcquisitionStd[!is.na(data3aoa$ageOfAcquisitionStd)]) - length(data4aoa$ageOfAcquisitionStd[!is.na(data4aoa$ageOfAcquisitionStd)])) / length(data3aoa$ageOfAcquisitionStd[!is.na(data3aoa$ageOfAcquisitionStd)])*100, digits=2); #2.75% aoa
round((length(data3ima$imageabilityStd[!is.na(data3ima$imageabilityStd)]) - length(data4ima$imageabilityStd[!is.na(data4ima$imageabilityStd)])) / length(data3ima$imageabilityStd[!is.na(data3ima$imageabilityStd)])*100, digits=2) #2.75% ima
round((length(data3conc$concretenessStd[!is.na(data3conc$concretenessStd)]) - length(data4conc$concretenessStd[!is.na(data4conc$concretenessStd)])) / length(data3conc$concretenessStd[!is.na(data3conc$concretenessStd)])*100, digits=2) #2.68% conc
round((length(data3cont$contextStd[!is.na(data3cont$contextStd)]) - length(data4cont$contextStd[!is.na(data4cont$contextStd)])) / length(data3cont$contextStd[!is.na(data3cont$contextStd)])*100, digits=2) #0.94%
round((length(data3pain$painfulnessStd[!is.na(data3pain$painfulnessStd)]) - length(data4pain$painfulnessStd[!is.na(data4pain$painfulnessStd)])) / length(data3pain$painfulnessStd[!is.na(data3pain$painfulnessStd)])*100, digits=2) #2.61
round((length(data3int$intensityStd[!is.na(data3int$intensityStd)]) - length(data4int$intensityStd[!is.na(data4int$intensityStd)])) / length(data3int$intensityStd[!is.na(data3int$intensityStd)])*100, digits=2) #3.98
round((length(data3unpl$unpleasantnessStd[!is.na(data3unpl$unpleasantnessStd)]) - length(data4unpl$unpleasantnessStd[!is.na(data4unpl$unpleasantnessStd)])) / length(data3unpl$unpleasantnessStd[!is.na(data3unpl$unpleasantnessStd)])*100, digits=2) #2.47
round((length(data3val$valenceStd[!is.na(data3val$valenceStd)]) - length(data4val$valenceStd[!is.na(data4val$valenceStd)])) / length(data3val$valenceStd[!is.na(data3val$valenceStd)])*100, digits=2) #2.87
round((length(data3aro$arousalStd[!is.na(data3aro$arousalStd)]) - length(data4aro$arousalStd[!is.na(data4aro$arousalStd)])) / length(data3aro$arousalStd[!is.na(data3aro$arousalStd)])*100, digits=2) #3.91
#distribution of valid datapoints across variables at this stage. Data necessary for figure 3.
c(sum(!is.na(data4fam$familiarityStd)), sum(!is.na(data4aoa$ageOfAcquisitionStd)), sum(!is.na(data4ima$imageabilityStd)), sum(!is.na(data4conc$concretenessStd)), sum(!is.na(data4cont$contextStd)), sum(!is.na(data4pain$painfulnessStd)), sum(!is.na(data4int$intensityStd)), sum(!is.na(data4unpl$unpleasantnessStd)), sum(!is.na(data4val$valenceStd)), sum(!is.na(data4aro$arousalStd)) ) -> temp1;
sum(temp1); #overall number of valid DP at this stage (figure in the paper) (243824)
sum(length(data3fam$familiarityStd[!is.na(data3fam$familiarityStd)]), length(data3aoa$ageOfAcquisitionStd[!is.na(data3aoa$ageOfAcquisitionStd)]), length(data3ima$imageabilityStd[!is.na(data3ima$imageabilityStd)]), length(data3conc$concretenessStd[!is.na(data3conc$concretenessStd)]), length(data3cont$contextStd[!is.na(data3cont$contextStd)]), length(data3pain$painfulnessStd[!is.na(data3pain$painfulnessStd)]), length(data3int$intensityStd[!is.na(data3int$intensityStd)]), length(data3unpl$unpleasantnessStd[!is.na(data3unpl$unpleasantnessStd)]), length(data3val$valenceStd[!is.na(data3val$valenceStd)]), length(data3aro$arousalStd[!is.na(data3aro$arousalStd)])); #overall number of valid DP pre-clustering, thus:250653
round((250653-243824)/250653*100, digits = 2); #overall 2.72% of DP taken out through the clustering
postscript("fig3.eps", onefile=F, family='Palatino');
par(mar=c(1,4,1,1)+.1);
pie(temp1, col=grey(seq(.2,1,length.out=10)), cex=2, labels=c('Familiarity','Age of acquisiton','Imageability','Concreteness','Context','Painfulness','Intensity','Unpleasantness','Valence','Arousal')); #figure 3 in the paper
dev.off();
############################
# reliability computations #
############################
#"We computed reliability for each variable by calculating the average split???half correlation over 1,000 random replicates, separately for each list. Results have an overall very strong reliability, as shown in Figure 4."
#familiarity
splitHalfFam <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4fam, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$familiarityStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$familiarityStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfFam[i,j];
print(i);
}
print(j);
};
###################
#age of acquisition
splitHalfAoa <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4aoa, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$ageOfAcquisitionStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$ageOfAcquisitionStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfAoa[i,j];
print(i);
}
print(j);
};
#############
#imageability
splitHalfIma <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4ima, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$imageabilityStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$imageabilityStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfIma[i,j];
print(i);
}
print(j);
};
#############
#concreteness
splitHalfConc <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4conc, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$concretenessStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$concretenessStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfConc[i,j];
print(i);
}
print(j);
};
########
#context
splitHalfCont <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4cont, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$contextStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$contextStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfCont[i,j];
print(i);
}
print(j);
};
boxplot(splitHalfCont);
########
#valence
splitHalfVal <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4val, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$valenceStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$valenceStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfVal[i,j];
print(i);
}
print(j);
};
boxplot(splitHalfVal);
########
#arousal
splitHalfAro <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4aro, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$arousalStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$arousalStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfAro[i,j];
print(i);
}
print(j);
};
boxplot(splitHalfAro);
############
#painfulness
splitHalfPainful <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4pain, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$painfulnessStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$painfulnessStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfPainful[i,j];
print(i);
}
print(j);
};
boxplot(splitHalfPainful);
##########
#intensity
splitHalfInt <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4int, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$intensityStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$intensityStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfInt[i,j];
print(i);
}
print(j);
};
boxplot(splitHalfInt);
###############
#unpleasantness
splitHalfUnpl <- matrix(nrow=1000, ncol=20);
for (j in 1:20)
{
temp <- subset(data4unpl, list==j);
for (i in 1:1000)
{
group1 <- sample(unique(temp$subject), size=floor(length(unique(temp$subject))/2));
group2 <- unique(temp$subject)[!(unique(temp$subject) %in% group1)];
temp1 <- subset(temp, subject %in% group1);
temp2 <- subset(temp, subject %in% group2);
aggregate(temp1$unpleasantnessStd, list(temp1$word), mean) -> temp3;
aggregate(temp2$unpleasantnessStd, list(temp2$word), mean) -> temp4;
cor(temp3$x, temp4$x, use='pairwise.complete.obs') -> splitHalfUnpl[i,j];
print(i);
}
print(j);
};
boxplot(splitHalfUnpl);
#now that we have split-half coefficients for each variable, we make figure 4 for the paper:
splitHalfOverall <- matrix( c(colSums(splitHalfFam)/1000, colSums(splitHalfAoa)/1000, colSums(splitHalfIma)/1000, colSums(splitHalfConc)/1000, colSums(splitHalfCont)/1000, colSums(splitHalfVal)/1000, colSums(splitHalfAro)/1000, colSums(splitHalfPainful)/1000, colSums(splitHalfInt)/1000, colSums(splitHalfUnpl)/1000), nrow=20, byrow=F);
colnames(splitHalfOverall) <- c('Familiarity','Age of acquisition','Imageability','Concreteness','Context','Valence','Arousal','Painfulness','Intensity','Unpleasantness');
postscript("fig4.eps", onefile=F, family='Palatino');
par(mar=c(3,4,1,1)+.1);
boxplot(splitHalfOverall, bty='n',outline=F, ylim=c(0,1), ylab="Split-half reliability");
dev.off();
##################
# draw the means #
##################
#familiarity
#Ready to draw the means
aggregate(data4fam$familiarityStd[is.na(data4fam$familiarityStd)==F], list(data4fam$word[is.na(data4fam$familiarityStd)==F]), mean) -> database;
names(database) <- c("word","meanFam");
nrow(database);#511, so one word missing. Which one?
data$word[!(data$word %in% database$word)]; #VIVO. Why is that?
nrow(subset(data3fam, word=="vivo"));#it's in 'data4'. So what's going on?
nrow(subset(data3fam, word=="vivo" & is.na(data3fam$familiarityStd)==F)); #Alright, so familiarityStd was always NA. Uhm...
summary(subset(data3fam, word=="vivo")); #familiarity is always NA, a bit weird. I take a look at the original dataset; NA there too. Ok, so I add the word with a NA -- do it while merging frequency so as to avoid format problems with the data frame.
database$knownFam <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4fam, word==database$word[i]);
database$knownFam[i] <- nrow(subset(temp, knownFam=="known"))/nrow(temp);
print(i);
};#not many bad percents
###################
#age of acquisition
aggregate(data4aoa$ageOfAcquisitionStd[is.na(data4aoa$ageOfAcquisitionStd)==F], list(data4aoa$word[is.na(data4aoa$ageOfAcquisitionStd)==F]), mean) -> temp;
names(temp) <- c("word","meanAoa");
nrow(temp);#511, so one word missing. Which one?
database$word[!(database$word %in% temp$word)]; #SPASMO.
summary(subset(data4aoa, word=="spasmo")); #it's in data4aoa
summary(subset(data, word=="spasmo")); #and in the original data, but with all NA. So, this must come from some technical error in the recording of the responses.
#Let's merge now:
names(temp);
names(database);
dim(database);
dim(merge(database,temp,by="word",all=T)); #we want 'vivo' in here, so 'all=T', not 'all.x=T'.
merge(database,temp,by="word",all=T) -> temp1;
subset(temp1, word=="spasmo");
subset(temp1, word=="vivo"); #ok, all fine
merge(database,temp,by="word",all=T) -> database;
database$knownAoA <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4aoa, word==database$word[i]);
database$knownAoA[i] <- nrow(subset(temp, knownAoA=="known"))/nrow(temp);
print(i);
};#not many bad percents
hist(database$knownAoA); #this is very good
subset(database, word=="spasmo");
subset(database, word=="vivo"); #cool, all fine
#############
#imageability
aggregate(data4ima$imageabilityStd[is.na(data4ima$imageabilityStd)==F], list(data4ima$word[is.na(data4ima$imageabilityStd)==F]), mean) -> temp;
names(temp) <- c("word","meanIma");
nrow(temp);#All words in, very good
#Let's merge now:
names(temp);
names(database);
dim(database);
dim(merge(database,temp,by="word",all=T));
subset(merge(database,temp,by="word",all=T), word=='vivo'); #ok, all fine
merge(database,temp,by="word",all=T) -> database;
database$knownIma <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4ima, word==database$word[i]);
database$knownIma[i] <- nrow(subset(temp, knownIma=="known"))/nrow(temp);
print(i);
};#not many bad percents
hist(database$knownIma); #very good!
#############
#concreteness
aggregate(data4conc$concretenessStd[is.na(data4conc$concretenessStd)==F], list(data4conc$word[is.na(data4conc$concretenessStd)==F]), mean) -> temp;
names(temp) <- c("word","meanConc");
nrow(temp);#All words in, very good
#Let's merge now:
names(temp);
names(database);
dim(database);
dim(merge(database,temp,by="word",all.x=T))
merge(database,temp,by="word",all.x=T) -> database;
database$knownConc <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4conc, word==database$word[i]);
database$knownConc[i] <- nrow(subset(temp, knownConc=="known"))/nrow(temp);
print(i);
};#not many bad percents
hist(database$knownConc); #very good!
#a little double check half way
summary(database); #all fine
########
#context
aggregate(data4cont$contextStd[is.na(data4cont$contextStd)==F], list(data4cont$word[is.na(data4cont$contextStd)==F]), mean) -> temp;
names(temp) <- c("word","meanCont");
nrow(temp);#All words in, very good
#Let's merge now:
names(temp);
names(database);
dim(database);
dim(merge(database,temp,by="word",all.x=T));
merge(database,temp,by="word",all.x=T) -> database;
database$knownCont <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4cont, word==database$word[i]);
database$knownCont[i] <- nrow(subset(temp, knownCont=="known"))/nrow(temp);
print(i);
};#not many bad percents
hist(database$knownCont); #very good!
########
#valence
aggregate(data4val$valenceStd[is.na(data4val$valenceStd)==F], list(data4val$word[is.na(data4val$valenceStd)==F]), mean) -> temp;
names(temp) <- c("word","meanVal");
nrow(temp);#510, so two words missing. Which ones?
database$word[!(database$word %in% temp$word)]; #CRAMPIFORME and SCHIACCIAMENTO. Let's take a look at the original dataset:
summary(subset(data4val, word=="crampiforme" | word=="schiacciamento")); #all NA, so must be some technical problem in the data gathering
#Let's merge now:
names(temp);
names(database);
dim(database);
merge(database,temp,by="word",all.x=T) -> temp1;
dim(temp1);
subset(temp1, word=="crampiforme"); #all right, we can overwrite
merge(database,temp,by="word",all.x=T) -> database;
database$knownVal <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4val, word==database$word[i]);
database$knownVal[i] <- nrow(subset(temp, knownVal=="known"))/nrow(temp);
print(i);
};
hist(database$knownVal);#very good, with a few exceptions
########
#arousal
aggregate(data4aro$arousalStd[is.na(data4aro$arousalStd)==F], list(data4aro$word[is.na(data4aro$arousalStd)==F]), mean) -> temp;
names(temp) <- c("word","meanAro");
nrow(temp);#All words present, nice!
#Let's merge now:
names(temp);
names(database);
dim(database);
merge(database,temp,by="word",all.x=T) -> temp1;
dim(temp1); #all right, we can overwrite
merge(database,temp,by="word",all.x=T) -> database;
database$knownAro <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4aro, word==database$word[i]);
database$knownAro[i] <- nrow(subset(temp, knownAro=="known"))/nrow(temp);
print(i);
};#not many bad percents
hist(database$knownAro);#very good!
############
#painfulness
aggregate(data4pain$painfulnessStd[is.na(data4pain$painfulnessStd)==F], list(data4pain$word[is.na(data4pain$painfulnessStd)==F]), mean) -> temp;
names(temp) <- c("word","meanPain");
nrow(temp);#All words present, nice!
#Let's merge now:
names(temp);
names(database);
dim(database);
merge(database,temp,by="word",all.x=T) -> temp1;
dim(temp1);
merge(database,temp,by="word",all.x=T) -> database;
database$knownPain <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4pain, word==database$word[i]);
database$knownPain[i] <- nrow(subset(temp, knownPain=="known"))/nrow(temp);
print(i);
};
hist(database$knownPain);#very good!
#another half-way doublecheck
summary(database); #all fine
##########
#intensity
aggregate(data4int$intensityStd[is.na(data4int$intensityStd)==F], list(data4int$word[is.na(data4int$intensityStd)==F]), mean) -> temp;
names(temp) <- c("word","meanInt");
nrow(temp);
#Let's merge now:
names(temp);
names(database);
dim(database);
dim(merge(database,temp,by="word",all.x=T));
merge(database,temp,by="word",all.x=T) -> database;
database$knownInt <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4int, word==database$word[i]);
database$knownInt[i] <- nrow(subset(temp, knownInt=="known"))/nrow(temp);
print(i);
};
hist(database$knownInt);#very good!
subset(database, knownInt<.70);#All nice, always the same very low-frequency words come up
###############
#unpleasantness
#Ready to draw the means
aggregate(data4unpl$unpleasantnessStd[is.na(data4unpl$unpleasantnessStd)==F], list(data4unpl$word[is.na(data4unpl$unpleasantnessStd)==F]), mean) -> temp;
names(temp) <- c("word","meanUnpl");
nrow(temp);#All words present, nice!
#Let's merge now:
names(temp);
names(database);
dim(database);
merge(database,temp,by="word",all.x=T) -> temp1;
dim(temp1);
summary(temp1); #all right, we can overwrite
merge(database,temp,by="word",all.x=T) -> database;
database$knownUnpl <- c(); #this will collect percent of known responses
for (i in 1:nrow(database))
{
temp <- subset(data4unpl, word==database$word[i]);
database$knownUnpl[i] <- nrow(subset(temp, knownUnpl=="known"))/nrow(temp);
print(i);
};
hist(database$knownUnpl);#very good!
subset(database, knownUnpl<.70);#All nice, always the same very low-frequency words come up
#write out the split-half objects -- they're time-taking to compute
save(file='splitHalf.RData', list=ls()[32:42]);
#clean up the workspace
rm(list=ls()[!ls() %in% c('database', 'data2')]);
##########################################################
# import SUBLTEX-IT and compute corpus-related variables #
##########################################################
subtlex <- read.table('subtlex-it.txt', header=T);
#############################
# explore the distributions #
#############################
postscript("fig5.eps", onefile=F, family='Palatino');
par(mar=c(2,.5,3,.5)+.1);
par(mfrow=c(5,2));
hist(database$meanFam, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Familiarity', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanFam, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanFam, na.rm=T), lwd=2, col = "red");
hist(database$meanAoa, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Age of Acqusition', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanAoa, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanAoa, na.rm=T), lwd=2, col = "red");
hist(database$meanIma, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Imageability', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanIma, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanIma, na.rm=T), lwd=2, col = "red");
hist(database$meanConc, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Concreteness', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanConc, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanConc, na.rm=T), lwd=2, col = "red");
hist(database$meanCont, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Context Availability', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanCont, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanCont, na.rm=T), lwd=2, col = "red");
hist(database$meanVal, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Valence', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanVal, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanVal, na.rm=T), lwd=2, col = "red");
hist(database$meanAro, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Arousal', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanAro, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanAro, na.rm=T), lwd=2, col = "red");
hist(database$meanPain, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Pain-relatedness', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanPain, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanPain, na.rm=T), lwd=2, col = "red");
hist(database$meanInt, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Intensity', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanInt, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanInt, na.rm=T), lwd=2, col = "red");
hist(database$meanUnpl, bty='n', breaks=seq(-2.7,2.5,.2), axes=F, xlab='', ylab='', main='Unpleasantness', col=grey(.80), border='black');
axis(1, at=seq(-2.5,2.5,.5), labels=c('-2.5','','-1.5','','-.5','','.5','','1.5','','2.5'));
abline(v=mean(database$meanUnpl, na.rm=T), lwd=2, col = "royalblue");
abline(v=median(database$meanUnpl, na.rm=T), lwd=2, col = "red");
legend(x = "topright",
c("Mean", "Median"),
col = c("royalblue", "red"),
lwd = c(2, 2))
dev.off();
#######################################
# explore the correlational structure #
#######################################
names(database);
library(corrplot);
corMatrix <- round(cor(database[,c(seq(2,22,2),23:ncol(database))], use="pairwise.complete.obs"), digits=2);
# Pilot 3 BLP scores
setwd("C:/Users/annal/OneDrive/Documents/GitHub/affixproject/Pilot3_data");
BLP_data3 <- read.table("BLP_preprocessed_pilot3.csv",header=T,sep=",");
summary(BLP_data3);
BLP_data3$AttentionCheck
